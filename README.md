## Illegal Immigrant Prediction Using Python

This Jupyter Notebook contains a Python script that predicts the number of illegal immigrants in the United States based on data from the year 2000 to 2016. The data is sourced from the U.S. Customs and Border Protection website and includes information on the number of illegal immigrants apprehended at the border by sector and state/territory.

## Data

The data used in this script includes the following columns:

- Border
- Sector
- State/Territory
- 2000 (All Illegal Immigrants)
- 2000 (Mexicans Only)
- 2001 (All Illegal Immigrants)
- 2001 (Mexicans Only)
- 2002 (All Illegal Immigrants)
- 2002 (Mexicans Only)
- 2003 (All Illegal Immigrants)
- 2003 (Mexicans Only)
- 2004 (All Illegal Immigrants)
- 2004 (Mexicans Only)
- 2005 (All Illegal Immigrants)
- 2005 (Mexicans Only)
- 2006 (All Illegal Immigrants)
- 2006 (Mexicans Only)
- 2007 (All Illegal Immigrants)
- 2007 (Mexicans Only)
- 2008 (All Illegal Immigrants)
- 2008 (Mexicans Only)
- 2009 (All Illegal Immigrants)
- 2009 (Mexicans Only)
- 2010 (All Illegal Immigrants)
- 2010 (Mexicans Only)
- 2011 (All Illegal Immigrants)
- 2011 (Mexicans Only)
- 2012 (All Illegal Immigrants)
- 2012 (Mexicans Only)
- 2013 (All Illegal Immigrants)
- 2013 (Mexicans Only)
- 2014 (All Illegal Immigrants)
- 2014 (Mexicans Only)
- 2015 (All Illegal Immigrants)
- 2015 (Mexicans Only)
- 2016 (All Illegal Immigrants)
- 2016 (Mexicans Only)


## Usage

To run the script, follow these steps:

1. Clone the repository to your local machine.
2. Open the Jupyter Notebook file Illegal_Immigrant_Prediction.ipynb in your preferred environment (e.g. Jupyter Notebook, JupyterLab).
3. Run each cell of the notebook in sequential order.
4. The final output will be the predicted number of illegal immigrants for each state/territory in the year 2017.


## License

This project is licensed under the MIT License. See the LICENSE file for more information.

## Acknowledgments

This script was developed as part of a data analysis project for Ben Lai. Special thanks to Data Scientist Gary Leung for his guidance and support throughout the project.
